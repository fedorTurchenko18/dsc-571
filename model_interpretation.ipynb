{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from service.app_api.features.extractor import FeatureExtractor\n",
    "from configs import utils\n",
    "from configs import settings\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import wandb\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    customers, sales = joblib.load('customers.joblib'), joblib.load('sales.joblib')\n",
    "except:\n",
    "    customers, sales = pd.read_excel('ucy_eko_data.xlsx', sheet_name='smile_customers'), pd.read_excel('ucy_eko_data.xlsx', sheet_name='smile_sales')\n",
    "    joblib.dump(customers, 'customers.joblib')\n",
    "    joblib.dump(sales, 'sales.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureExtractor(target_month=3, perform_split=True, generation_type='categorical', filtering_set='customers', period=60, subperiod=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = fe.transform(sales=sales, customers=customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(f\"{settings.SETTINGS['WANDB_ENTITY']}/Cat-BoostClassifier/tkq50baj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.loads(run.json_config)\n",
    "params = {}\n",
    "for param in config:\n",
    "    params.update({param: config[param]['value']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(**params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(Pool(X_test, y_test, cat_features=params['cat_features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shapley_scatter(colname: str, shap_values: np.ndarray, data: pd.DataFrame, ncols: int):\n",
    "    '''\n",
    "    `colname` - initial column name (without `FeatureExtractor.subperiod` slicing)\n",
    "    `shap_values` - array of shapley values\n",
    "    `data` - training data frame, same shape as shap_values\n",
    "    `ncols` - how many columns to fit charts in\n",
    "    '''\n",
    "    df_chart = pd.DataFrame(\n",
    "        data={\n",
    "            col: data.loc[:, col].values for col in data.columns if colname in col\n",
    "        }\n",
    "    )\n",
    "    # data_type = df_chart.iloc[:, 0].dtype\n",
    "    sub_plots = df_chart.shape[1]\n",
    "    rows = sub_plots // ncols\n",
    "    pos = range(1, sub_plots + 1)\n",
    "    if sub_plots % ncols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    for i in range(sub_plots):\n",
    "        ax = fig.add_subplot(rows, ncols, pos[i])\n",
    "        current_feature = df_chart.iloc[:, i].name\n",
    "        df_chart['shapley'] = shap_values[:, data.columns.tolist().index(current_feature)]\n",
    "        if df_chart[current_feature].nunique() > 10:\n",
    "            sns.scatterplot(data=df_chart, x=current_feature, y='shapley', ax=ax)\n",
    "        else:\n",
    "            sns.stripplot(data=df_chart, x=current_feature, y='shapley', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_scatter('segments', shap_values, X_test, ncols=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers of a \"passerby\" segment were more likely to be classified as churned. On the other hand, being in the churn risk group, does not itself imply that the customer is likely to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_scatter('last_purchase_share', shap_values, X_test, ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each subperiod there was computed a share of last purchase (in terms of monetary value) from all the purchases made during this subperiod. For example, if a customer made 4 purchases with `qty` 25 each during the subperiod, then the `last_purchase_share = 0.25`.\n",
    "- Day 1 to 15: High last purchase share implies that customer made a few purchases or even a single one during the very first days of activity. Such customers are more likely to churn due to this \"passerby\" kind of a consuming behavior. On the other hand, low last purchase share during first 15 days of being a client means that there will be quite some purchases in the subsequent periods.\n",
    "- Day 16 to 30: Customers with a low, but non-zero (meaning that they actually purchased at least something), last purchase share are more likely to churn than to stay. It means that they made the majority of their purchases during first weeks. They made an insignificant purchase closer to the end of second 15 day period of activity, which could imply their overall dissatisfaction with the services.\n",
    "- Day 31 to 45: If the last purchase share is at least around 10%, then it is quite likely for such customers to stay for the subsequent month. However, it is also important to mention that here the factor of making at least a single purchase during the second month of activity could influence such outcome.\n",
    "- Day 46 to 60: The last purchase share of more than 50% during the end of a second month of activity implies, in fact, lower chance of completing at least a purchase next month. Yet, it is still only a sign of diminishing purchasing activity but not an absolute churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_scatter('prodcatbroad', shap_values, X_test, ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each subperiod, there was computed a number of purchases in unique categories. For example, if a customer at least once bought fuel and coffee, then `prodcatbroad_<subperiod> = 2`.\n",
    "- Day 1 to 15: Purchases in any number of unique categories do not make a significant impact on probability of churn during the first 15 days of activity\n",
    "- Day 16 to 30: 0 purchases during the second 15 day period of activity is a weak signal of potential churn. As for days 1 to 15, any non-zero number of unique categories, in which the purchase was made, does not significantly affect the resistance to churn\n",
    "- Day 31 to 45: Crucial subperiod in this feature's terms. Even if, during this subperiod, the purchase was made in at least a single unique category, the customer is likely not to churn. While 0 purchases during this period is a strong indicator of churn\n",
    "- Day 46 to 60: This feature has no impact at the subperiod of last 15 days within 60 days training data slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster.elbow import KElbowVisualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_model = KMeans()\n",
    "elbow_viz = KElbowVisualizer(clust_model, k=(1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_viz.fit(shap_values)\n",
    "elbow_viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = KMeans(3, random_state=571)\n",
    "visualizer = SilhouetteVisualizer(best_model, colors='yellowbrick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.fit(shap_values)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = best_model.fit_predict(shap_values)\n",
    "X_shap = X_test.copy()\n",
    "X_shap['shap_clustering_labels'] = labels\n",
    "X_shap['target'] = y_test\n",
    "X_shap['predicted_target'] = model.predict(X_test)\n",
    "probas = model.predict_proba(X_test)\n",
    "X_shap['proba_0'] = probas[:, 0]\n",
    "X_shap['proba_1'] = probas[:, 1]\n",
    "X_shap['predicted_probability'] = X_shap.apply(lambda x: x['proba_0'] if x['predicted_target']==0 else x['proba_1'], axis=1)\n",
    "X_shap['prediction_accuracy'] = X_shap.apply(lambda x: 'error' if x['target']!=x['predicted_target'] else 'correct', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shap['shap_clustering_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_shap.groupby('shap_clustering_labels')['target'].value_counts(normalize=True)*100).sort_index().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shap.groupby('shap_clustering_labels')['predicted_probability'].describe()[['mean', '50%']].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average predictions confidence in shapley values cluster 2 is significantly lower than in two other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_shap.groupby('shap_clustering_labels')['prediction_accuracy'].value_counts(normalize=True)*100).sort_index().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite expectedly, the model has made most of errors in the ambigous cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_shap.groupby('shap_clustering_labels')['segments'].value_counts(normalize=True)*100).sort_index().to_frame('segment_share')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 2 is composed of a mixture of various segments without any particular one really standing out, as it is the case for two other clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
